  根本问题是反向传播要优化的代价函数。对于一个给定的神经网络，代价函数仍然是单一值，单一的目的。因此，传统的神经网络固有的不能记忆、遗忘和选择，
像LSTMs所做的一样。
  rnn不能解决长期依赖的问题。当当前任务与以前相关的信息距离很远时，rnn不能连接这个信息。而LSTM能学习长期依赖。
  LSTMs明确的被设计来解决长期依赖性问题。记住时间长期的信息实际上是他们默认的行为，而不是LSTMs尽力去做的。
  LSTMs核心思想：
  LSTMs的关键是单元状态（cell state，贯穿图顶端的水平线）。单元状态就像一种传送带，它贯穿整条链，仅带有一些线性交互。信息非常容易不被改变的沿着它流动。
  LSTM有能力通过门（gates）这种结构的调整向单元状态移除或者添加信息。门是一种让信息有选择地通过的一种方式。它由一个sigmoid神经网络层和一个点乘操作构成。
  sigmoid层输出值在0——1之间，用来描述每个组件的多少应该被通过。当为0时，不让任何通过；当为1时，让所有通过。
  一个LSTM有3个门，来保护和控制单元状态。
  LSTM步骤：1、遗忘旧信息；2、存储新信息；3、输出。每个步骤都有一个门。
参考文献：https://colah.github.io/posts/2015-08-Understanding-LSTMs/


  run与cnn：
  cnn的一个明显缺点是它的api有太多的限制：它接受一个固定大小的向量作为输入（例如图片），并产生一个固定大小的向量作为输出（例如不同类的概率）。另外，cnn
模型进行这些映射时使用的是固定数量的计算阶段（例如模型中层的数量）。
  rnn的最大的优点是rnn允许我们对向量序列操作，比如输入序列、输出序列、或者两者都操作。
  即使输入输出是固定向量，使用序列的方式来处理它也是有可能的。
