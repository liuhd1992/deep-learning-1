{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec之Skip-Gram模型-中文文本版\n",
    "\n",
    "下面代码将用TensorFlow实现Word2Vec中的Skip-Gram模型。\n",
    "\n",
    "关于Skip-Gram模型请参考上一篇[知乎专栏文章](https://zhuanlan.zhihu.com/p/27234078)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 加载数据\n",
    "\n",
    "数据集使用的是来自Matt Mahoney的维基百科文章，数据集已经被清洗过，去除了特殊符号等，并不是全量数据，只是部分数据，所以实际上最后训练出的结果很一般（语料不够）。\n",
    "\n",
    "如果想获取更全的语料数据，可以访问以下网站，这是gensim中Word2Vec提供的语料：\n",
    "\n",
    "- 来自Matt Mahoney预处理后的[文本子集](http://mattmahoney.net/dc/enwik9.zip)，里面包含了10亿个字符。\n",
    "- 与第一条一样的经过预处理的[文本数据](http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2)，但是包含了30个亿的字符。\n",
    "- 多种语言的[训练文本](http://www.statmt.org/wmt11/translation-task.html#download)。\n",
    "- [UMBC webbase corpus](http://ebiquity.umbc.edu/redirect/to/resource/id/351/UMBC-webbase-corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/Javasplittedwords') as f:\n",
    "    text = f.read()\n",
    "words = text.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 数据预处理\n",
    "\n",
    "数据预处理过程主要包括：\n",
    "\n",
    "- 替换文本中特殊符号并去除低频词\n",
    "- 对文本分词\n",
    "- 构建语料\n",
    "- 单词映射表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['熟练掌握', 'java', '熟悉', 'python', 'shell', '熟练使用', 'git', 'svn', '能够', '发现']\n",
      "word_count 16940\n"
     ]
    }
   ],
   "source": [
    "print(words[0:10])\n",
    "# 筛选低频词  为什么要删除低频词？\n",
    "words_count = Counter(words)\n",
    "print('word_count', words_count['java'])\n",
    "words = [w for w in words if words_count[w] > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建映射表\n",
    "vocab = set(words)\n",
    "#由于vocab是一个set，因此每次重启kernel后运行代码，w，c的映射关系会不同。而且vocab中不会有重复的单词\n",
    "vocab_to_int = {w: c for c, w in enumerate(vocab)}  ##返回一个w为key，c为value的字典\n",
    "int_to_vocab = {c: w for c, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 8614953\n",
      "unique words: 6846\n"
     ]
    }
   ],
   "source": [
    "print(\"total words: {}\".format(len(words)))\n",
    "print(\"unique words: {}\".format(len(set(words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374\n"
     ]
    }
   ],
   "source": [
    "# 对原文本进行vocab到int的转换\n",
    "int_words = [vocab_to_int[w] for w in words]   #因为是便利words，所以是对原文本\n",
    "int_java = vocab_to_int['java']\n",
    "print(int_java)  ##即用2402这个int型数来代表‘java’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 采样\n",
    "\n",
    "对停用词进行采样，例如“the”， “of”以及“for”这类单词进行剔除。剔除这些单词以后能够加快我们的训练过程，同时减少训练过程中的噪音。\n",
    "\n",
    "我们采用以下公式:\n",
    "$$ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}} $$\n",
    "\n",
    "其中$ t $是一个阈值参数，一般为1e-3至1e-5。  \n",
    "$f(w_i)$ 是单词 $w_i$ 在整个数据集中的出现频次。  \n",
    "$P(w_i)$ 是单词被删除的概率。\n",
    "\n",
    ">这个公式和论文中描述的那个公式有一些不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n"
     ]
    }
   ],
   "source": [
    "t = 1e-5 # t值\n",
    "threshold = 0.9 # 剔除概率阈值\n",
    "\n",
    "# 统计单词出现频次\n",
    "int_word_counts = Counter(int_words)\n",
    "print(int_word_counts[2402])   #2402代表’java‘，与words_count['java'])结果都为16940\n",
    "total_count = len(int_words)  \n",
    "# 计算单词频率  \n",
    "word_freqs = {w: c/total_count for w, c in int_word_counts.items()}  ##dict.items用来遍历字典的key和value\n",
    "# 计算被删除的概率\n",
    "prob_drop = {w: 1 - np.sqrt(t / word_freqs[w]) for w in int_word_counts}\n",
    "# 对单词进行采样\n",
    "train_words = [w for w in int_words if prob_drop[w] < threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3905916"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 构造batch\n",
    "\n",
    "Skip-Gram模型是通过输入词来预测上下文。因此我们要构造我们的训练样本，具体思想请参考知乎专栏，这里不再重复。\n",
    "\n",
    "对于一个给定词，离它越近的词可能与它越相关，离它越远的词越不相关，这里我们设置窗口大小为5，对于每个训练单词，我们还会在[1:5]之间随机生成一个整数R，用R作为我们最终选择output word的窗口大小。这里之所以多加了一步随机数的窗口重新选择步骤，是为了能够让模型更聚焦于当前input word的邻近词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_targets(words, idx, window_size=5):\n",
    "    '''\n",
    "    获得input word的上下文单词列表\n",
    "    \n",
    "    参数\n",
    "    ---\n",
    "    words: 单词列表(一个batch）\n",
    "    idx: input word的索引号\n",
    "    window_size: 窗口大小\n",
    "    '''\n",
    "    target_window = np.random.randint(1, window_size+1)\n",
    "    # 这里要考虑input word前面单词不够的情况\n",
    "    start_point = idx - target_window if (idx - target_window) > 0 else 0\n",
    "    end_point = idx + target_window\n",
    "    # output words(即窗口中的上下文单词) 一个窗口单词个数是 1 + target_window *2\n",
    "    targets = set(words[start_point: idx] + words[idx+1: end_point+1]) #因为start_point 取值不确定所以要分开写\n",
    "    return list(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(words, batch_size, window_size=5):\n",
    "    '''\n",
    "    构造一个获取batch的生成器\n",
    "    '''\n",
    "    n_batches = len(words) // batch_size\n",
    "    \n",
    "    # 仅取full batches\n",
    "    words = words[:n_batches*batch_size]  ##超出n_batches*batch_size的部分被舍弃\n",
    "    print('words[0:1]', words[0:1])\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx: idx+batch_size]   ##一个batch的len始终是batch_size\n",
    "        for i in range(len(batch)):\n",
    "            batch_x = batch[i]   ##i的取值范围刚好是0～batch_size\n",
    "            batch_y = get_targets(batch, i, window_size)\n",
    "            # 由于一个input word会对应多个output word，因此需要长度统一\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "            y.extend(batch_y)\n",
    "       # print('x[0:1]=', x[0:1])\n",
    "       # print('len(x)', len(x))\n",
    "      #  print('y[0:1]=', y[0:1] )\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 构建网络\n",
    "\n",
    "该部分主要包括：\n",
    "\n",
    "- 输入层\n",
    "- Embedding\n",
    "- Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    inputs = tf.placeholder(tf.int32, shape=[None], name='inputs')\n",
    "    labels = tf.placeholder(tf.int32, shape=[None, None], name='labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "嵌入矩阵的矩阵形状为 $ vocab\\_size\\times hidden\\_units\\_size$ \n",
    "\n",
    "TensorFlow中的tf.nn.embedding_lookup函数可以实现lookup的计算方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6846\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(int_to_vocab)\n",
    "print(vocab_size)\n",
    "embedding_size = 200 # 嵌入维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    # 嵌入层权重矩阵\n",
    "    embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n",
    "    # 实现lookup\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "\n",
    "负采样主要是为了解决梯度下降计算速度慢的问题，详情同样参考我的上一篇知乎专栏文章。\n",
    "\n",
    "TensorFlow中的tf.nn.sampled_softmax_loss会在softmax层上进行采样计算损失，计算出的loss要比full softmax loss低。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sampled = 100\n",
    "\n",
    "with train_graph.as_default():\n",
    "    softmax_w = tf.Variable(tf.truncated_normal([vocab_size, embedding_size], stddev=0.1))\n",
    "    softmax_b = tf.Variable(tf.zeros(vocab_size))\n",
    "    \n",
    "    # 计算negative sampling下的损失\n",
    "   #loss = tf.nn.sampled_softmax_loss(softmax_w, softmax_b, labels, embed, n_sampled, vocab_size)\n",
    "    loss = tf.nn.nce_loss(softmax_w, softmax_b, labels, embed, n_sampled, vocab_size)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证\n",
    "\n",
    "为了更加直观的看到我们训练的结果，我们将查看训练出的相近语义的词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  40   50   84 1039 1027 1025]\n",
      "[123, 997, 4895, 374, 2357, 5104, 4734]\n",
      "valid_dataset (7,)\n",
      "WARNING:tensorflow:From <ipython-input-15-13f59c4849f0>:24: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "normalized_embedding (6846, 200)\n",
      "valid_embedding (7, 200)\n",
      "(7, 6846)\n"
     ]
    }
   ],
   "source": [
    "with train_graph.as_default():\n",
    "    # 随机挑选一些单词\n",
    "    ## From Thushan Ganegedara's implementation\n",
    "    valid_size = 7 # Random set of words to evaluate similarity on.\n",
    "    valid_window = 100\n",
    "    # pick 8 samples from (0,100) and (1000,1100) each ranges. lower id implies more frequent \n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples, \n",
    "                               random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "    print(valid_examples)\n",
    "    valid_examples = [vocab_to_int['word'], \n",
    "                      vocab_to_int['ppt'], \n",
    "                      vocab_to_int['熟悉'],\n",
    "                      vocab_to_int['java'], \n",
    "                      vocab_to_int['能力'], \n",
    "                      vocab_to_int['逻辑思维'],\n",
    "                      vocab_to_int['了解']]\n",
    "    print(valid_examples)\n",
    "    valid_size = len(valid_examples)\n",
    "    # 验证单词集\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    print('valid_dataset', valid_dataset.shape)\n",
    "    # 计算每个词向量的模并进行单位化\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "    normalized_embedding = embedding / norm\n",
    "    print('normalized_embedding', normalized_embedding.shape)\n",
    "    # 查找验证单词的词向量\n",
    "    valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)\n",
    "    print('valid_embedding', valid_embedding.shape)\n",
    "    # 计算余弦相似度(验证的6个单词的词向量与整个语料库6846个单词的词向量的相似度)\n",
    "    similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))\n",
    "    print(similarity.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object get_batches at 0x7f753d50d7d8>\n",
      "words[0:1] [2093]\n",
      "Epoch 1/1 Iteration: 100 Avg. Training loss: 298.9031 0.0835 sec/batch\n",
      "Epoch 1/1 Iteration: 200 Avg. Training loss: 281.6870 0.0703 sec/batch\n",
      "Epoch 1/1 Iteration: 300 Avg. Training loss: 236.1611 0.0746 sec/batch\n",
      "Epoch 1/1 Iteration: 400 Avg. Training loss: 200.3986 0.0695 sec/batch\n",
      "Epoch 1/1 Iteration: 500 Avg. Training loss: 171.7265 0.0655 sec/batch\n",
      "Epoch 1/1 Iteration: 600 Avg. Training loss: 143.0911 0.0695 sec/batch\n",
      "Epoch 1/1 Iteration: 700 Avg. Training loss: 112.2309 0.0752 sec/batch\n",
      "Epoch 1/1 Iteration: 800 Avg. Training loss: 95.5553 0.0693 sec/batch\n",
      "Epoch 1/1 Iteration: 900 Avg. Training loss: 88.6673 0.0683 sec/batch\n",
      "Epoch 1/1 Iteration: 1000 Avg. Training loss: 74.1558 0.0640 sec/batch\n",
      "[-0.10314064 -0.15883324 -0.03447697]\n",
      "Nearest to [word]: 乐观, 整理, 团建, python, 待遇, 自信, 扎实, 求知欲,\n",
      "[-0.28697851 -0.12442343  0.00517113]\n",
      "Nearest to [ppt]: 内部, 福利待遇, 文字, 经理, 准确, 国内, 完整, 程序,\n",
      "[-0.04279021 -0.02088061  0.04817206]\n",
      "Nearest to [熟悉]: 可用, 剪辑, 开发者, 开发资源, 适合, 电子商务, 塞班岛, 资历,\n",
      "[-0.05320531 -0.03968342 -0.09781852]\n",
      "Nearest to [java]: 挖掘, 武汉, 传输, 假日, 谦虚, 话题, 挫折, 制作,\n",
      "[-0.01790917 -0.04124239  0.05094324]\n",
      "Nearest to [能力]: 以下, 形成, 与众不同, 系统化, 出具, 提取, 平均年龄, 中餐,\n",
      "[ -3.27074736e-01  -3.97183895e-02  -1.14751980e-04]\n",
      "Nearest to [逻辑思维]: 资料, 责任, 文字, 清晰, 数据结构, 挑战, 程序, 旅游,\n",
      "[ 0.00120926 -0.07042921  0.02776987]\n",
      "Nearest to [了解]: 下方, 关系数据库, 直接参与, 人数, 功能测试, 次数, 一行, 商场,\n",
      "Epoch 1/1 Iteration: 1100 Avg. Training loss: 61.9262 0.0673 sec/batch\n",
      "Epoch 1/1 Iteration: 1200 Avg. Training loss: 52.2245 0.0650 sec/batch\n",
      "Epoch 1/1 Iteration: 1300 Avg. Training loss: 48.3042 0.0668 sec/batch\n",
      "Epoch 1/1 Iteration: 1400 Avg. Training loss: 44.2937 0.0649 sec/batch\n",
      "Epoch 1/1 Iteration: 1500 Avg. Training loss: 33.1964 0.0665 sec/batch\n",
      "Epoch 1/1 Iteration: 1600 Avg. Training loss: 33.1121 0.0646 sec/batch\n",
      "Epoch 1/1 Iteration: 1700 Avg. Training loss: 36.6309 0.0606 sec/batch\n",
      "Epoch 1/1 Iteration: 1800 Avg. Training loss: 27.0012 0.0654 sec/batch\n",
      "Epoch 1/1 Iteration: 1900 Avg. Training loss: 21.7817 0.0635 sec/batch\n",
      "Epoch 1/1 Iteration: 2000 Avg. Training loss: 19.4807 0.0665 sec/batch\n",
      "[-0.35524857 -0.26026571 -0.17874925]\n",
      "Nearest to [word]: 整理, 乐观, 待遇, 功底, 团建, 扎实, 语言表达, 影响力,\n",
      "[-0.45742765 -0.22397561 -0.13821766]\n",
      "Nearest to [ppt]: 内部, 福利待遇, 采编, 准确, 文字, 国内, 意见, 完整,\n",
      "[-0.0644873  -0.03385788  0.02734586]\n",
      "Nearest to [熟悉]: 可用, 开发资源, 剪辑, 开门红, 开发者, 塞班岛, 适合, 资历,\n",
      "[-0.04881611 -0.04109463 -0.09869272]\n",
      "Nearest to [java]: 挖掘, 武汉, 传输, 假日, 谦虚, 制作, 挫折, 碰,\n",
      "[-0.06700293 -0.06650084  0.01224024]\n",
      "Nearest to [能力]: 以下, 形成, 与众不同, 系统化, 提取, 出具, 积极探索, 中餐,\n",
      "[-0.50134706 -0.15649347 -0.15239276]\n",
      "Nearest to [逻辑思维]: 资料, 政策, 英语, 更新, 媒体, 文字, 责任, 商务,\n",
      "[-0.02136925 -0.08047111  0.01013706]\n",
      "Nearest to [了解]: 下方, 关系数据库, 直接参与, 人数, 次数, 功能测试, 一行, 商场,\n",
      "Epoch 1/1 Iteration: 2100 Avg. Training loss: 18.0116 0.0675 sec/batch\n",
      "Epoch 1/1 Iteration: 2200 Avg. Training loss: 14.8579 0.0739 sec/batch\n",
      "Epoch 1/1 Iteration: 2300 Avg. Training loss: 14.4889 0.0701 sec/batch\n",
      "Epoch 1/1 Iteration: 2400 Avg. Training loss: 12.4451 0.0678 sec/batch\n",
      "Epoch 1/1 Iteration: 2500 Avg. Training loss: 12.3808 0.0648 sec/batch\n",
      "Epoch 1/1 Iteration: 2600 Avg. Training loss: 9.9093 0.0692 sec/batch\n",
      "Epoch 1/1 Iteration: 2700 Avg. Training loss: 11.0162 0.0727 sec/batch\n",
      "Epoch 1/1 Iteration: 2800 Avg. Training loss: 10.3162 0.0717 sec/batch\n",
      "Epoch 1/1 Iteration: 2900 Avg. Training loss: 8.5091 0.0714 sec/batch\n",
      "Epoch 1/1 Iteration: 3000 Avg. Training loss: 9.4920 0.0682 sec/batch\n",
      "[-0.36948985 -0.35132635 -0.21591148]\n",
      "Nearest to [word]: 乐观, 团建, 整理, 待遇, 功底, 影响力, python, 扎实,\n",
      "[-0.47164172 -0.31897321 -0.17929551]\n",
      "Nearest to [ppt]: 内部, 福利待遇, 支撑, 采编, 完整, 国内, 意见, 准确,\n",
      "[-0.06400325 -0.04570467  0.02195382]\n",
      "Nearest to [熟悉]: 可用, 开发资源, 开门红, 剪辑, 塞班岛, 资历, 开发者, 植入,\n",
      "[-0.04840413 -0.04179972 -0.09817934]\n",
      "Nearest to [java]: 挖掘, 武汉, 传输, 制作, 假日, 挫折, 谦虚, 碰,\n",
      "[-0.06714578 -0.09004057  0.00251339]\n",
      "Nearest to [能力]: 以下, 与众不同, 形成, 系统化, 提取, 出具, 积极探索, 中餐,\n",
      "[-0.48998341 -0.24740204 -0.18279567]\n",
      "Nearest to [逻辑思维]: 政策, 资料, 英语, 正, 更新, 精准, 责任, visio,\n",
      "[-0.02104276 -0.08820777  0.00584922]\n",
      "Nearest to [了解]: 下方, 关系数据库, 直接参与, 人数, 次数, 一行, 商场, 功能测试,\n",
      "Epoch 1/1 Iteration: 3100 Avg. Training loss: 8.9175 0.0758 sec/batch\n",
      "Epoch 1/1 Iteration: 3200 Avg. Training loss: 7.8609 0.0781 sec/batch\n",
      "Epoch 1/1 Iteration: 3300 Avg. Training loss: 7.9771 0.0684 sec/batch\n",
      "Epoch 1/1 Iteration: 3400 Avg. Training loss: 6.9369 0.0685 sec/batch\n",
      "Epoch 1/1 Iteration: 3500 Avg. Training loss: 5.6332 0.0738 sec/batch\n",
      "Epoch 1/1 Iteration: 3600 Avg. Training loss: 6.7526 0.0753 sec/batch\n",
      "Epoch 1/1 Iteration: 3700 Avg. Training loss: 6.0825 0.0709 sec/batch\n",
      "Epoch 1/1 Iteration: 3800 Avg. Training loss: 6.0174 0.0706 sec/batch\n",
      "Epoch 1/1 Iteration: 3900 Avg. Training loss: 5.6464 0.0725 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 1 # 迭代轮数\n",
    "batch_size = 1000 # batch大小\n",
    "window_size = 10 # 窗口大小\n",
    "\n",
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver() # 文件存储\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        #带有yield关键字的函数返回一个生成器\n",
    "        batches = get_batches(train_words, batch_size, window_size)\n",
    "        print(batches)\n",
    "        start = time.time()\n",
    "        # for循环会自动调用next函数\n",
    "        for x, y in batches:\n",
    "            \n",
    "            feed = {inputs: x,\n",
    "                    labels: np.array(y)[:, None]}\n",
    "            train_loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "            loss += train_loss\n",
    "            \n",
    "            if iteration % 100 == 0: \n",
    "                end = time.time()\n",
    "                print(\"Epoch {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Avg. Training loss: {:.4f}\".format(loss/100),\n",
    "                      \"{:.4f} sec/batch\".format((end-start)/100))\n",
    "                loss = 0\n",
    "                start = time.time()\n",
    "            \n",
    "            # 计算相似的词\n",
    "            if iteration % 1000 == 0:\n",
    "                # 计算similarity\n",
    "                sim = similarity.eval()\n",
    "                for i in range(valid_size):\n",
    "                    valid_word = int_to_vocab[valid_examples[i]]\n",
    "                    top_k = 8 # 取最相似单词的前8个 sim前取负号是因为sim越大，相关性越高。则-sim越小，相关新越高\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k+1] #0的位置是valid_word本身\n",
    "                    log = 'Nearest to [%s]:' % valid_word\n",
    "                    for k in range(top_k):\n",
    "                        close_word = int_to_vocab[nearest[k]]\n",
    "                        log = '%s %s,' % (log, close_word)\n",
    "                    print(log)\n",
    "            \n",
    "            iteration += 1\n",
    "            \n",
    "    save_path = saver.save(sess, \"checkpoints/text8.ckpt\")\n",
    "    embed_mat = sess.run(normalized_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
